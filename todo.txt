General
* for batchnorm, need to know whether you're training or not; set train=false when making graph, =true when train() called, =false at end of train()
    * How to get this into the Layer classes?
* add __str__ and __repr__ for the Layers classes
* ? Get some hyperparameter tuning going
* Multi-gpu training?


Miniplaces
* ? Update the loss function so that if you predict the correct label as, e.g., the second most likely class, your loss is smaller than if you predict it as being much less likely. This could help us to do better on acc@5; it might give a better signal during training too? I'm not sure.
* ? make sure that separate model classes (e.g. PretrainedCNN vs CNN) save to a different log_key. Perhaps add a log_key suffix or prefix that the model uses? So instead of "default", it would use "default_cnn" or "default_pretrainedcnn". This isn't strictly necessary, but otherwise we probably ought to at least store something that says which CNN class was used
* Make a ResNet or make the images at least 197x197 to try keras.ResNet50
* Look at dilated convolution (large kernels w/ small # parameters)
    * How does this compare to separating the kernels into, e.g., [1, 7] then [7, 1]?
* Try turning on the pooling after the CNN modules (for the pretrained ones)
* Dataset augmentation
* Make named models instead of run_num? It could be nicer in tensorboard
* Add visualizations of the trained network
* What is tf.nn.local_response_normalization? Similar to batch norm?
* Look at SDD for object detection; we'd need to train it from scratch, though. See what their data augmentation methods were; these apparently helped.
* ? Try to use Dataset
* If reloading a model, when you call train, early_stop_loss should be set to the dev_loss that was stored in the log. Otherwise, you'll end up always saving the model after the first training iteration, even if that was worse than the previously saved one.
* Plot loss more often than once/epoch? Instantaneous loss for that batch? What about dev loss? For a random sample of the dev data? But only use the loss at the end of an epoch for early stopping?


VQA
* Try to train the dense layers fully then retrain with finetuning
